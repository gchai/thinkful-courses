# Asymptotic analysis

**1.5 hours** Average completion time

#### What you'll learn

###### By the end of this lesson, you will be able to use big O notation to represent the order of growth of the running time of an algorithm.

#### Overview

###### Analyzing an algorithm means determining the resources that the algorithm will consume during execution. Sometimes, the resource in question is the memory of the computer or the network bandwidth. However, the most important resource to measure is the computational time. That is, how long does it take to execute the algorithm on a given set of inputs?

## Key Terms

Rate of growth

Also called the order of growth, the rate at which the running time of an algorithm increases as a function of the input size

Big O notation

A notation commonly used to describe the order of growth of an algorithm

## A model of computation

Today, computers come in all shapes and sizes, from small wearable devices, to laptop and desktop machines, to massive servers and supercomputers running many CPUs in parallel. Even with one class of devices, there are so many differences in CPU technology that it would be impossible to list them all. Some CPUs have a single core, while many CPUs today come with multiple cores, allowing software to take advantage of asynchronous execution models. Many computers include additional processing units like GPUs (graphics processing units) where some of the work can be offloaded, freeing up the main CPU. And this doesn't even touch on the different memory models, cache pipelines, bus speeds, and other variations and techniques used to provide all the computing power that you may need.

With all this variation, it is very difficult to isolate the efficiency of the algorithm itself.

Rather than depend on the computer's implementation details, it is common to adopt a simplified model of the computer for the purpose of analysis. What is really being measured is the amount of work that is required to execute the algorithm. The fact that a faster computer will do the work faster does not change the fact that the same amount of work is being done.

The computational model that you'll adopt here has the following properties:

1. It has a single processor and runs the algorithm in a sequential manner. That is, the instructions for the algorithm are executed in the given order, one at a time.
    
2. It takes exactly one time unit to execute a standard instruction. A standard instruction is an operation such as addition, subtraction, multiplication, division, comparisons, assignments, and conditional control. No complex multistep instructions (such as sorting) can be done in a single time unit.
    
3. It takes the same amount of space to store each value (such as an integer).
    
4. It has an infinitely large memory. This assumption frees you from worrying about the space requirements.
    

A _time unit_ is a generic amount of time. Because of variations in the speed of processors, some can execute an instruction in a few nanoseconds, while others may take a few milliseconds to execute the same instruction. Here, focus on how many of the time units are used, rather than the absolute time.

## Counting steps

Recall the problem that was solved in the previous lesson:

Given an integer n, find the sum of all integers from 1 to n inclusively. For example, if n=4, the solution involves the sum 1+2+3+4=10.

And recall the first solution that was presented:

`function sumIntegers(n) {` `let sum = 0;` `for (let i = 1; i <= n; i++) {` `sum = sum + i;` `}` `return sum;`

`}`

How many steps would it take to execute this algorithm on the computer model described above? First, break up this program into its individual steps. A diagram might be helpful here:

![](https://images.ctfassets.net/c7lxnbtvvcxm/1ejYqbhFSGuTkKzq44kHYb/2f16320d0f86cd429bd07da842104882/Eng-dsa_intro_008.png)

In this flow chart, each statement to be executed is represented by a rectangle or diamond. A diamond shape represents a conditional statement.

Starting from the top, look at each statement in turn. Write down the number of times that statement will be executed when the algorithm runs.

For example, the first statement, sum=0, will only execute once. The value of n doesn't change this. The next statement, i=1 is the loop initialization step, and it is only executed once at the beginning of the loop.

The loop condition statement, i<=n, depends on the value of n. Consider a few small examples. Suppose that n is 1. This condition executes once when i is equal to 1 and one more time when i is equal to 2. When n is 1, this statement executes twice.

Suppose that n was 2. The loop condition executes once when i is equal to 1, once when i is equal to 2, and once when i is equal to 3. Therefore, when n is 2, this statement executes three times.

Generalizing that, you can say that this statement executed n+1 times. You should try a few more examples, say when n is equal to 4 and when n is equal to 5, to see if this conclusion holds up.

![](https://images.ctfassets.net/c7lxnbtvvcxm/1VbQdJPojbnSC67327m7Qf/f4900d953ca3cd465734e04aa4772277/Eng-dsa_intro_009.png)

The next statement occurs in the body of the loop. Everything in the body of the loop depends on the number of times that the loop iterates, which in turn depends on n. Like the previous statement, you can use a few small examples to try to determine the pattern.

|n|Number of executions|Explanation|
|---|---|---|
|1|Executes one time|When i is 1, i<=n is true. So the loop body executes, and i is then incremented to 2. That means that i<=n becomes false and the loop ends.|
|2|Executes two times|Similar to above, but when i is 2, i<=n is still true. So the loop body executes a second time. i is then incremented to 3 and i<=n becomes false.|

You can do a few more of these to verify the pattern. Generally, it looks like the statement executes n times.

The next statement, the loop step, executes n times, by the same logic as the previous step. The final statement, return sum, executes just once.

![](https://images.ctfassets.net/c7lxnbtvvcxm/21hUpkcZhox53pS7GefK9Q/e21a40d3bc7e36bdb3af2c6fe231d2e3/Eng-dsa_intro_010.png)

To put it all together, sum the number of steps:

`1 + 1 + n + 1 + n + n + 1 = 3n + 4`

Therefore, you can say that the running time for this algorithm in terms of the input n is 3n+4.

Using this function, you can calculate the number of steps that this algorithm takes for any value of n. For example, if n is equal to 10, this algorithm will execute as follows:

`3 * 10 + 4 = 34 steps`

And if n is equal to 100, this algorithm will execute as follows:

`3 * 100 + 4 = 304 steps`

## Order of growth

So far, some simplifying assumptions have been made to facilitate this analysis. For example, a simple computer model was assumed. Next, the actual time to execute an instruction was ignored. In reality, computers take slightly longer to execute multiplication than addition. Here, that difference is ignored.

The execution time derived above, 3n+4, contains more information than is needed, and further simplifications can be made.

It is the rate of growth that you are really interested in. The rate of growth, also called the order of growth, is the rate at which the running time of an algorithm increases as a function of the input size.

For this reason, it is only the highest-order term in the running time that matters. To illustrate this, consider the running time 3n+4. There are two terms in this expression: 3n and 4. As n grows, the 3n term dominates the value of the expression.

|n|3n|4|3n+4|Contribution of 4 to the value|
|---|---|---|---|---|
|1|3|4|7|57.000%|
|10|30|4|34|11.760%|
|100|300|4|304|1.310%|
|1,000|3,000|4|3,004|0.013%|

As you can see, as n increases to ever-larger values, the 4 in the expression has less effect on the final value. You could simply drop that from the expression, leaving only the 3n.

It is also possible to drop the leading term's constant coefficient, because constant factors are less significant than the growth rate. That just leaves n.

You can say that the algorithm has a growth rate of n. This is written as O(n), pronounced "big Oh of n."

Big O is a notation commonly used to describe the order of growth of an algorithm. Specifically, it describes the upper bound, or worst-case running time of the algorithm.

## Big O

How can big O notation be interpreted? What exactly does O(n) mean?

O(f(n)) describes a set of functions that grow at most as fast as the function f(n). For example, O(n) refers to the function f(n)=n. You can plot that on a graph, as shown here:

![](https://images.ctfassets.net/c7lxnbtvvcxm/6J9Yo6hCS7xHINvsoMZA3r/cb94c899aa43d4cf6a2ae3f50541d9e9/Eng-dsa_intro_011.png)

The red line represents the function f(n)=n. It simply means that as n increases, the number of steps increases proportionally (at the same rate).

O(n) then means any function that grows slower than or at the same rate as the function f(n).

For example, in the following graph, the function g(n) grows as n increases. But it remains under the f(n) line.

![](https://images.ctfassets.net/c7lxnbtvvcxm/77AXAMgoIaZwjYrNesIQUh/9ec26f4e7334fcf60cf2652715efc9ae/Eng-dsa_intro_012.png)

If g(n) were the running time of an algorithm, you could say that g(n) is O(n).

Notice also that O(f(n)) is true for sufficiently large values of n. It may be possible that for very small values, the running time is larger than the O(f(n)) function. As long as for large values of n, as n approaches infinity, it remains less than or equal to f(n), and it is still a valid definition. For this reason, you can call this an asymptotic analysis.

Because f(n)=n is a straight line, you can say that this is a linear function and O(n) is a linear growth rate.

## Growth rate of the second solution

A second solution was provided for the sum of n numbers problem in the previous lesson. Review that solution below:

`function sumIntegers2(n) {` `return (n * (n + 1)) / 2;`

`}`

You can use a similar process as before to count the number of steps that it takes to complete this algorithm.

First, a flow chart of the function will help visualize the steps. Even though the entire function is written in a single expression, there are multiple operations happening. Remember that the model computer can execute only one operation at a time.

![](https://images.ctfassets.net/c7lxnbtvvcxm/5EkUlW1QyvnwtgBDDX1Epx/027f06870940997863e38087a807d8ca/Eng-dsa_intro_013.png)

For illustration purposes, only a temporary variable i was introduced to show the order of execution of the expression.

How many times does each of these statements execute? Does it depend on the value of n? It is clear that there are no loop structures in this program, so each statement executes exactly once—regardless of the value of n. That gives a running time of 4.

The order of growth of this solution is O(1), because 1 represents the constant function when all constants are factored out.

![](https://images.ctfassets.net/c7lxnbtvvcxm/BIaLC5hC1vKffMHPZg3kB/671aa628778731d18e09de046f13944a/Eng-dsa_intro_014.png)

Once again, O(1) simply means that the actual running time of this algorithm grows at most as fast as the constant function.

This is called a constant growth rate.

## The insert-into-a-sorted-array problem

The video below provides a brief introduction to solving an algorithm using an optimal approach.

Consider the following problem: You are given an array of n numbers. The first n-1 numbers in the array are sorted in ascending order, but the status of the last number is unknown. That is, the last number in the array may or may not be in the right position. Find the correct position for that last number and insert it into the array such that the array becomes fully sorted.

How might you solve such a problem?

Here is an instance of the problem:

![](https://images.ctfassets.net/c7lxnbtvvcxm/6ShFVpE1QIBGUKPZPYXupP/b8dcf79a72681f083fc333d665c71159/Eng-dsa_intro_015.png)

In the first step, initialize a loop variable to n-2. This will keep track of the value that you are currently looking at in the array.

![](https://images.ctfassets.net/c7lxnbtvvcxm/6xe0xZs6b0WhkYeR5lSOt8/3c1568456359eed7f5ca7454d38d2ddc/Eng-dsa_intro_016.png)

Create a new variable named key to hold the value in the last position of the array.

Compare the current value to key. If the current value is greater than key, then copy the current value to position i+1.

![](https://images.ctfassets.net/c7lxnbtvvcxm/7E7OHhWEf4ECJjylWAhqnH/ac0c36bb34abd1ea875b6e66159ac934/Eng-dsa_intro_017.png)

Repeat until the current value is not greater than key:

![](https://images.ctfassets.net/c7lxnbtvvcxm/1sy1X8rFK0QlTdH5UZJfvU/5479f9e7b47d7092c654df38d8dad172/Eng-dsa_intro_018.png)

When you find a position that is less than key, copy key into the i+1 position of the array.

To take care of the situation where key is the smallest number in the array, if you get to the front of the array without inserting the key, simply copy the key into the first location of the array.

Those steps may be formalized into a function as follows:

`function insertIntoSortedArray(sequence) {` `const key = sequence[sequence.length - 1];` `for (let i = sequence.length - 2; i >= 0; i--) {` `if (sequence[i] > key) {` `sequence[i + 1] = sequence[i];` `} else {` `sequence[i + 1] = key;` `return sequence;` `}` `}` `sequence[0] = key;` `return sequence;`

`}`

## Runtime of the insert-into-sorted-array solution

To count the number of steps that this algorithm takes, you'll follow a similar process as in the previous two examples. However, there is one significant difference.

The number of steps depends on both the size of the input array and the values in the array. For example, consider what happens if the input array is [3, 5, 7, 10, 12]. The loop will iterate only once before this algorithm ends. On the other hand, given the input [3, 5, 7, 10, 2], the loop will iterate at least n times.

So, how can you determine the running time of this algorithm?

There are three possible scenarios here. The first example above, where the loop iterates only once, is the best-case scenario. That is, the algorithm has little or no work to do. This case has a constant running time. Although this is a desirable situation, it isn't very interesting. It doesn't give you any information that is useful to the analysis.

The worst case is the scenario where the algorithm has to do the most amount of work. Like the second example above, if the key must be inserted at the beginning of the array, the only way to do that is to iterate the entire array. That gives you a running time of n.

The third possibility is the average case. If the algorithm is executed repeatedly with many different inputs, it will sometimes do a single iteration and sometimes do n iterations. It would be possible to say that on average, it will do n/2 iterations. Because constant coefficients are dropped in asymptotic analysis, this is the same as n.

Even though the average case can give you some meaningful data about the real-world complexity of the algorithm, it doesn't give you a growth rate that is different from the worst case. Think of it as a budgeting problem. To make an effective budget, you need to know the highest possible cost of everything that you need to purchase.

You can conclude that the growth rate of the running time of this algorithm is O(n).

## Other running times

In the examples in this lesson, you saw two examples of growth rates: O(n) and O(1). It is possible to use any function in big O notation, but it is common practice to use a few well-defined functions. In the next lesson, some of these common functions will be explored in more detail.