# Space complexity

**1.5 hours** Average completion time

#### What you'll learn

###### By the end of this lesson, you will be able to determine the space complexity of a code sample.

#### Overview

###### So far, you have only learned about the time complexity of the algorithms. That is, you've learned about the worst-case (big O) amount of time that it takes for the algorithm to complete the task. However, it's also important to pay attention to the amount of space that the algorithm takes to complete the task. The space complexity of an algorithm relates to how much memory the algorithm uses.

## Key Terms

Auxiliary space

The temporary or extra space used by the algorithm while it is being executed

## [](https://github.com/Thinkful-Ed/web-dev-programs/blob/master/library/intro-dsa-06-space-complexity/content.md#understanding-space-complexity)Understanding space complexity

Space complexity is a measure of the amount of working storage that an algorithm needs. It answers the following question: in the worst case, how much memory is needed at any point in the algorithm?

Space complexity includes both auxiliary space and the space used by the input. Auxiliary space is the temporary or extra space used by the algorithm while it is being executed.

As with time complexity, you are mostly concerned with how the space needs grow, in big O terms, as the size of the input grows. Below are a few examples of expressing space complexity using big O notation, starting from slowest space growth (best) to fastest (worst):

- O(1): Constant complexity means that the algorithm takes the same amount of space regardless of the input size.
    
- O(log n): Logarithmic complexity means that the algorithm takes space proportional to the log of the input size.
    
- O(n): Linear complexity means that the algorithm takes space directly proportional to the input size.
    
- O(n log n) Log-linear or quasilinear complexity (also called linearithmic) means that the space complexity grows proportionally to the input size and a logarithmic factor.
    
- O(n²): Quadratic complexity means that the space complexity grows proportionally to the square of the input size.
    

The space complexity analysis works similarly to time complexity.

## [](https://github.com/Thinkful-Ed/web-dev-programs/blob/master/library/intro-dsa-06-space-complexity/content.md#constant-space-o1)Constant space (O(1))

To get warmed up, consider a simple function that sums two numbers:

`function sum(left, right) {` `return left + right;`

`}`

In this particular function, three variables are used and allocated in memory:

- The first parameter, left
    
- The second parameter, right
    
- The return value
    

In JavaScript, a single number occupies eight bytes of memory. In the above function, there are three variables assumed to be numbers. Therefore, this algorithm always takes 24 bytes of memory to complete (3 * 8 bytes). As a result, the space complexity is constant, so it can be expressed in big O notation as O(1).

## [](https://github.com/Thinkful-Ed/web-dev-programs/blob/master/library/intro-dsa-06-space-complexity/content.md#logarithmic-space-olog-n)Logarithmic space (O(log n))

Logarithmic space complexity (O(log n)) is the next best thing after constant space. Although logarithmic space complexity algorithms do require more space with larger inputs, space usage increases slowly. For example, imagine that you have a function, quickSort(), which takes 128 bytes to process an input of size 10. When you increase the input by 10 times, to 100, the space required only grows to 256 bytes. When you increase the input size to 1,000, the space only grows to 384 bytes.

It is also characteristic of logarithmic algorithms that they cut the problem size in half each round through.

As you may have guessed, quicksort uses O(log n) space. You will learn more about quicksort in a later lesson.

## [](https://github.com/Thinkful-Ed/web-dev-programs/blob/master/library/intro-dsa-06-space-complexity/content.md#linear-space-on)Linear space (O(n))

Algorithms with linear space complexity (O(n)) use an amount of space directly proportional to the size of the input. Every time that you double the size of the input, the algorithms will require twice as much memory.

For example, look at the space complexity of a function that sums all of the elements in an array:

`function sum(numbers) {` `let total = 0;` `for (const number of numbers) {` `total += number;` `}` `return total;`

`}`

Again, list all variables present in the above code:

- numbers: The space taken by the array is equal to 8n bytes, where n is the length of the array
    
- number: An 8-byte number
    
- total : An 8-byte number
    

The total space needed for this algorithm is 8n+8+8 bytes. The highest order of n in this equation is just n. Thus, the space complexity of the above is O(n).

## [](https://github.com/Thinkful-Ed/web-dev-programs/blob/master/library/intro-dsa-06-space-complexity/content.md#space-time-complexity-tradeoff)Space-time complexity tradeoff

An algorithm's efficiency is a combination of its time and space complexity. So an efficient algorithm is one that is fast and that takes the least amount of memory possible.

Space and time complexity are often linked. Usually, increasing speed leads to greater memory consumption, and vice versa. However, this isn't always the case; sometimes, space and time complexity aren't correlated. For example, bubble sort is a slow algorithm that occupies minimal space. On the other hand, merge sort (which you will learn about later) is an extremely fast algorithm that requires a lot of memory.

There are also some balanced sorting algorithms. In these, the speed and space usage aren't the best, but they are acceptable in most cases.